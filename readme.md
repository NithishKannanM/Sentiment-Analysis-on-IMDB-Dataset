Project Overview

This project performs binary sentiment analysis (positive/negative) on the IMDB movie reviews dataset. The goal is to compare the performance of a classical machine learning model (SVM) with a deep learning model (CNN-LSTM).

The workflow includes:

Text preprocessing (cleaning, lemmatization, stopword removal)

Feature extraction using TF-IDF for SVM and tokenization + padding for deep learning

Training models and tuning hyperparameters

Evaluating models using standard metrics

Comparing model performance visually

1. Data Cleaning/Preprocessing

Checked the data and need to be cleaned

Convert text to lowercase

Remove HTML tags, URLs, numbers, and punctuation

Expand common English contractions (e.g., "can't" → "cannot")

Tokenize text and remove stopwords (noice)

Lemmatize words to their dictionary base form

2. Feature Engineering

For SVM (classical ML):

TF-IDF Vectorizer: Converts text into numerical vectors reflecting word importance

Limited vocabulary: max_features=5000

Stopwords removed automatically

For CNN-LSTM (deep learning):

Tokenizer: Converts text into sequences of integers

Maximum vocabulary size: max_words=10000

Sequence padding to a fixed length (max_len=250)

Out-of-vocabulary words replaced with <OOV>

3. Model Architectures
Support Vector Machine (SVM)

LinearSVC classifier

Input: TF-IDF vectors

Binary classification with sigmoid threshold 0.5

CNN-LSTM

Embedding Layer: 64-dimensional word embeddings

Conv1D + MaxPooling1D: Extracts local n-gram features

LSTM Layer: Captures sequential dependencies

Dropout + L2 Regularization: Prevents overfitting

Optimizer: Adam (learning rate = 1e-4)

Callbacks: ReduceLROnPlateau + EarlyStopping for better convergence

4. Evaluation Metrics

I used standard classification metrics: Accuracy ,Precision, Recall, F1-score.

Confusion Matrix – Visualizes true positives, false positives, true negatives, false negatives

5. Results

Model	    Accuracy	Precision	Recall	F1-score
SVM	        0.88	    0.85	    0.90	0.87
CNN-LSTM	0.92	    0.91	    0.93	0.92

Observation: CNN-LSTM outperforms SVM in all metrics, indicating better handling of sequential dependencies and contextual information in reviews.

Visual comparison can be seen in the bar chart generated by the results_df.plot() function.

6. Conclusion

SVM provides a strong baseline using TF-IDF features but cannot capture word order or context.

CNN-LSTM leverages embeddings, convolution, and LSTM layers to understand local patterns and sequential context, achieving higher accuracy and F1-score.

Proper preprocessing, regularization, and early stopping significantly improve deep learning performance.

7. Future Work

Experiment with pretrained embeddings like GloVe or FastText

Use Bidirectional LSTM or Transformer-based models (BERT, DistilBERT) for better context understanding

Hyperparameter tuning for batch size, learning rate, and network architecture

Data augmentation for handling imbalanced datasets